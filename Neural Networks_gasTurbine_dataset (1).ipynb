{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your first MLP in Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "#seed = 7\n",
    "#numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"C:\\Games\\Bhag Milkha Bhag 2013 Hindi MC DVDScr Xvid AC3 IcTv\\Desktop\\CSV\\gas_turbines.csv\",delimiter= \",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "#X = dataset[:,0:8]\n",
    "#Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.70</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>114.71</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>111.61</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>111.78</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>110.19</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>110.74</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>111.58</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     TEY     CDP  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  114.70  10.605   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  114.72  10.598   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  114.71  10.601   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  114.72  10.606   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  114.72  10.612   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  111.61  10.400   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  111.78  10.433   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  110.19  10.483   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  110.74  10.533   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  111.58  10.583   \n",
       "\n",
       "           CO     NOX  \n",
       "0      3.1547  82.722  \n",
       "1      3.2363  82.776  \n",
       "2      3.2012  82.468  \n",
       "3      3.1923  82.670  \n",
       "4      3.2484  82.311  \n",
       "...       ...     ...  \n",
       "15034  4.5186  79.559  \n",
       "15035  4.8470  79.917  \n",
       "15036  7.9632  90.912  \n",
       "15037  6.2494  93.227  \n",
       "15038  4.9816  92.498  \n",
       "\n",
       "[15039 rows x 11 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AT      0\n",
       "AP      0\n",
       "AH      0\n",
       "AFDP    0\n",
       "GTEP    0\n",
       "TIT     0\n",
       "TAT     0\n",
       "TEY     0\n",
       "CDP     0\n",
       "CO      0\n",
       "NOX     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.00000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17.764381</td>\n",
       "      <td>1013.19924</td>\n",
       "      <td>79.124174</td>\n",
       "      <td>4.200294</td>\n",
       "      <td>25.419061</td>\n",
       "      <td>1083.798770</td>\n",
       "      <td>545.396183</td>\n",
       "      <td>134.188464</td>\n",
       "      <td>12.102353</td>\n",
       "      <td>1.972499</td>\n",
       "      <td>68.190934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.574323</td>\n",
       "      <td>6.41076</td>\n",
       "      <td>13.793439</td>\n",
       "      <td>0.760197</td>\n",
       "      <td>4.173916</td>\n",
       "      <td>16.527806</td>\n",
       "      <td>7.866803</td>\n",
       "      <td>15.829717</td>\n",
       "      <td>1.103196</td>\n",
       "      <td>2.222206</td>\n",
       "      <td>10.470586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.522300</td>\n",
       "      <td>985.85000</td>\n",
       "      <td>30.344000</td>\n",
       "      <td>2.087400</td>\n",
       "      <td>17.878000</td>\n",
       "      <td>1000.800000</td>\n",
       "      <td>512.450000</td>\n",
       "      <td>100.170000</td>\n",
       "      <td>9.904400</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>27.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.408000</td>\n",
       "      <td>1008.90000</td>\n",
       "      <td>69.750000</td>\n",
       "      <td>3.723900</td>\n",
       "      <td>23.294000</td>\n",
       "      <td>1079.600000</td>\n",
       "      <td>542.170000</td>\n",
       "      <td>127.985000</td>\n",
       "      <td>11.622000</td>\n",
       "      <td>0.858055</td>\n",
       "      <td>61.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18.186000</td>\n",
       "      <td>1012.80000</td>\n",
       "      <td>82.266000</td>\n",
       "      <td>4.186200</td>\n",
       "      <td>25.082000</td>\n",
       "      <td>1088.700000</td>\n",
       "      <td>549.890000</td>\n",
       "      <td>133.780000</td>\n",
       "      <td>12.025000</td>\n",
       "      <td>1.390200</td>\n",
       "      <td>66.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23.862500</td>\n",
       "      <td>1016.90000</td>\n",
       "      <td>90.043500</td>\n",
       "      <td>4.550900</td>\n",
       "      <td>27.184000</td>\n",
       "      <td>1096.000000</td>\n",
       "      <td>550.060000</td>\n",
       "      <td>140.895000</td>\n",
       "      <td>12.578000</td>\n",
       "      <td>2.160400</td>\n",
       "      <td>73.935500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>34.929000</td>\n",
       "      <td>1034.20000</td>\n",
       "      <td>100.200000</td>\n",
       "      <td>7.610600</td>\n",
       "      <td>37.402000</td>\n",
       "      <td>1100.800000</td>\n",
       "      <td>550.610000</td>\n",
       "      <td>174.610000</td>\n",
       "      <td>15.081000</td>\n",
       "      <td>44.103000</td>\n",
       "      <td>119.890000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AT           AP            AH          AFDP          GTEP  \\\n",
       "count  15039.000000  15039.00000  15039.000000  15039.000000  15039.000000   \n",
       "mean      17.764381   1013.19924     79.124174      4.200294     25.419061   \n",
       "std        7.574323      6.41076     13.793439      0.760197      4.173916   \n",
       "min        0.522300    985.85000     30.344000      2.087400     17.878000   \n",
       "25%       11.408000   1008.90000     69.750000      3.723900     23.294000   \n",
       "50%       18.186000   1012.80000     82.266000      4.186200     25.082000   \n",
       "75%       23.862500   1016.90000     90.043500      4.550900     27.184000   \n",
       "max       34.929000   1034.20000    100.200000      7.610600     37.402000   \n",
       "\n",
       "                TIT           TAT           TEY           CDP            CO  \\\n",
       "count  15039.000000  15039.000000  15039.000000  15039.000000  15039.000000   \n",
       "mean    1083.798770    545.396183    134.188464     12.102353      1.972499   \n",
       "std       16.527806      7.866803     15.829717      1.103196      2.222206   \n",
       "min     1000.800000    512.450000    100.170000      9.904400      0.000388   \n",
       "25%     1079.600000    542.170000    127.985000     11.622000      0.858055   \n",
       "50%     1088.700000    549.890000    133.780000     12.025000      1.390200   \n",
       "75%     1096.000000    550.060000    140.895000     12.578000      2.160400   \n",
       "max     1100.800000    550.610000    174.610000     15.081000     44.103000   \n",
       "\n",
       "                NOX  \n",
       "count  15039.000000  \n",
       "mean      68.190934  \n",
       "std       10.470586  \n",
       "min       27.765000  \n",
       "25%       61.303500  \n",
       "50%       66.601000  \n",
       "75%       73.935500  \n",
       "max      119.890000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15039 entries, 0 to 15038\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   AT      15039 non-null  float64\n",
      " 1   AP      15039 non-null  float64\n",
      " 2   AH      15039 non-null  float64\n",
      " 3   AFDP    15039 non-null  float64\n",
      " 4   GTEP    15039 non-null  float64\n",
      " 5   TIT     15039 non-null  float64\n",
      " 6   TAT     15039 non-null  float64\n",
      " 7   TEY     15039 non-null  float64\n",
      " 8   CDP     15039 non-null  float64\n",
      " 9   CO      15039 non-null  float64\n",
      " 10  NOX     15039 non-null  float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     CDP      CO  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  10.605  3.1547   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  10.598  3.2363   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  10.601  3.2012   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  10.606  3.1923   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  10.612  3.2484   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  10.400  4.5186   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  10.433  4.8470   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  10.483  7.9632   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  10.533  6.2494   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  10.583  4.9816   \n",
       "\n",
       "          NOX  \n",
       "0      82.722  \n",
       "1      82.776  \n",
       "2      82.468  \n",
       "3      82.670  \n",
       "4      82.311  \n",
       "...       ...  \n",
       "15034  79.559  \n",
       "15035  79.917  \n",
       "15036  90.912  \n",
       "15037  93.227  \n",
       "15038  92.498  \n",
       "\n",
       "[15039 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gas = dataset.drop('TEY',axis=1)\n",
    "gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "      <th>TEY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "      <td>114.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "      <td>114.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "      <td>114.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "      <td>114.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "      <td>114.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "      <td>111.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "      <td>111.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "      <td>110.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "      <td>110.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "      <td>111.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     CDP      CO  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  10.605  3.1547   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  10.598  3.2363   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  10.601  3.2012   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  10.606  3.1923   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  10.612  3.2484   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  10.400  4.5186   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  10.433  4.8470   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  10.483  7.9632   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  10.533  6.2494   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  10.583  4.9816   \n",
       "\n",
       "          NOX     TEY  \n",
       "0      82.722  114.70  \n",
       "1      82.776  114.72  \n",
       "2      82.468  114.71  \n",
       "3      82.670  114.72  \n",
       "4      82.311  114.72  \n",
       "...       ...     ...  \n",
       "15034  79.559  111.61  \n",
       "15035  79.917  111.78  \n",
       "15036  90.912  110.19  \n",
       "15037  93.227  110.74  \n",
       "15038  92.498  111.58  \n",
       "\n",
       "[15039 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gas_turbines = pd.concat([pd.DataFrame(gas),dataset[['TEY']]], axis = 1)\n",
    "gas_turbines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[114.7 ],\n",
       "       [114.72],\n",
       "       [114.71],\n",
       "       ...,\n",
       "       [110.19],\n",
       "       [110.74],\n",
       "       [111.58]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "array = gas_turbines.values\n",
    "X = array[:,0:10]\n",
    "Y = array[:,10]\n",
    "\n",
    "X.reshape(-1,1)\n",
    "Y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=10,  activation='relu'))\n",
    "model.add(Dense(8,  activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "1053/1053 [==============================] - 25s 2ms/step - loss: 18301.0052 - val_loss: 17209.2656\n",
      "Epoch 2/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18403.0043 - val_loss: 17209.2656\n",
      "Epoch 3/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18312.3840 - val_loss: 17209.2656\n",
      "Epoch 4/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18341.4104 - val_loss: 17209.2656\n",
      "Epoch 5/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18298.6344 - val_loss: 17209.2656\n",
      "Epoch 6/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18284.1042 - val_loss: 17209.2656\n",
      "Epoch 7/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18277.8817 - val_loss: 17209.2656\n",
      "Epoch 8/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18303.5880 - val_loss: 17209.2656\n",
      "Epoch 9/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18336.3907 - val_loss: 17209.2656\n",
      "Epoch 10/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18266.0198 - val_loss: 17209.2656\n",
      "Epoch 11/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18254.3218 - val_loss: 17209.2656\n",
      "Epoch 12/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18353.6290 - val_loss: 17209.2656\n",
      "Epoch 13/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18323.9885 - val_loss: 17209.2656\n",
      "Epoch 14/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18323.9680 - val_loss: 17209.2656\n",
      "Epoch 15/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18373.6426 - val_loss: 17209.2656\n",
      "Epoch 16/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18270.4620 - val_loss: 17209.2656\n",
      "Epoch 17/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18323.0858 - val_loss: 17209.2656\n",
      "Epoch 18/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18298.6053 - val_loss: 17209.2656\n",
      "Epoch 19/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18275.5120 - val_loss: 17209.2656\n",
      "Epoch 20/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18328.2046 - val_loss: 17209.2656\n",
      "Epoch 21/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18293.9131 - val_loss: 17209.2656\n",
      "Epoch 22/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18268.3268 - val_loss: 17209.2656\n",
      "Epoch 23/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18306.7802 - val_loss: 17209.2656\n",
      "Epoch 24/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18295.9270 - val_loss: 17209.2656\n",
      "Epoch 25/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18304.3018 - val_loss: 17209.2656\n",
      "Epoch 26/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18315.8996 - val_loss: 17209.2656\n",
      "Epoch 27/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18427.4150 - val_loss: 17209.2656\n",
      "Epoch 28/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18390.4126 - val_loss: 17209.2656\n",
      "Epoch 29/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18265.9743 - val_loss: 17209.2656\n",
      "Epoch 30/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18301.0699 - val_loss: 17209.2656\n",
      "Epoch 31/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18335.6842 - val_loss: 17209.2656\n",
      "Epoch 32/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18335.7137 - val_loss: 17209.2656\n",
      "Epoch 33/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18326.4101 - val_loss: 17209.2656\n",
      "Epoch 34/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18320.9695 - val_loss: 17209.2656\n",
      "Epoch 35/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18344.1895 - val_loss: 17209.2656\n",
      "Epoch 36/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18285.2314 - val_loss: 17209.2656\n",
      "Epoch 37/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18386.7115 - val_loss: 17209.2656\n",
      "Epoch 38/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18249.1900 - val_loss: 17209.2656\n",
      "Epoch 39/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18328.9898 - val_loss: 17209.2656\n",
      "Epoch 40/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18389.4453 - val_loss: 17209.2656\n",
      "Epoch 41/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18312.9034 - val_loss: 17209.2656\n",
      "Epoch 42/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18373.2096 - val_loss: 17209.2656\n",
      "Epoch 43/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18280.3152 - val_loss: 17209.2656\n",
      "Epoch 44/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18365.8168 - val_loss: 17209.2656\n",
      "Epoch 45/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18298.8705 - val_loss: 17209.2656\n",
      "Epoch 46/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18398.0013 - val_loss: 17209.2656\n",
      "Epoch 47/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18310.8826 - val_loss: 17209.2656\n",
      "Epoch 48/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18289.7902 - val_loss: 17209.2656\n",
      "Epoch 49/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18330.3937 - val_loss: 17209.2656\n",
      "Epoch 50/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18314.2129 - val_loss: 17209.2656\n",
      "Epoch 51/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18310.8191 - val_loss: 17209.2656\n",
      "Epoch 52/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18386.8867 - val_loss: 17209.2656\n",
      "Epoch 53/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18280.0746 - val_loss: 17209.2656\n",
      "Epoch 54/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18307.0382 - val_loss: 17209.2656\n",
      "Epoch 55/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18245.8093 - val_loss: 17209.2656\n",
      "Epoch 56/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18285.2910 - val_loss: 17209.2656\n",
      "Epoch 57/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18359.7973 - val_loss: 17209.2656\n",
      "Epoch 58/150\n",
      "1053/1053 [==============================] - 2s 1ms/step - loss: 18301.6833 - val_loss: 17209.2656\n",
      "Epoch 59/150\n",
      "1053/1053 [==============================] - 2s 2ms/step - loss: 18370.5664 - val_loss: 17209.2656\n",
      "Epoch 60/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18319.2924 - val_loss: 17209.2656\n",
      "Epoch 61/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18343.9852 - val_loss: 17209.2656\n",
      "Epoch 62/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18338.3004 - val_loss: 17209.2656\n",
      "Epoch 63/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18325.0679 - val_loss: 17209.2656\n",
      "Epoch 64/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18362.7454 - val_loss: 17209.2656\n",
      "Epoch 65/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18397.6551 - val_loss: 17209.2656\n",
      "Epoch 66/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18363.1339 - val_loss: 17209.2656\n",
      "Epoch 67/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18351.0992 - val_loss: 17209.2656\n",
      "Epoch 68/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18343.4771 - val_loss: 17209.2656\n",
      "Epoch 69/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18370.6948 - val_loss: 17209.2656\n",
      "Epoch 70/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18336.7637 - val_loss: 17209.2656\n",
      "Epoch 71/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18402.0925 - val_loss: 17209.2656\n",
      "Epoch 72/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18345.0416 - val_loss: 17209.2656\n",
      "Epoch 73/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18377.9877 - val_loss: 17209.2656\n",
      "Epoch 74/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18311.4019 - val_loss: 17209.2656\n",
      "Epoch 75/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18297.3109 - val_loss: 17209.2656\n",
      "Epoch 76/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18291.3414 - val_loss: 17209.2656\n",
      "Epoch 77/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18360.0285 - val_loss: 17209.2656\n",
      "Epoch 78/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18406.3820 - val_loss: 17209.2656\n",
      "Epoch 79/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18296.6610 - val_loss: 17209.2656\n",
      "Epoch 80/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18319.3554 - val_loss: 17209.2656\n",
      "Epoch 81/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18344.8502 - val_loss: 17209.2656\n",
      "Epoch 82/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18313.9670 - val_loss: 17209.2656\n",
      "Epoch 83/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18337.2181 - val_loss: 17209.2656\n",
      "Epoch 84/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18386.3858 - val_loss: 17209.2656\n",
      "Epoch 85/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18372.4337 - val_loss: 17209.2656\n",
      "Epoch 86/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18330.6809 - val_loss: 17209.2656\n",
      "Epoch 87/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18321.1840 - val_loss: 17209.2656\n",
      "Epoch 88/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18329.9821 - val_loss: 17209.2656\n",
      "Epoch 89/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18199.4243 - val_loss: 17209.2656\n",
      "Epoch 90/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18350.9845 - val_loss: 17209.2656\n",
      "Epoch 91/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18369.4765 - val_loss: 17209.2656\n",
      "Epoch 92/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18338.9013 - val_loss: 17209.2656\n",
      "Epoch 93/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18254.6576 - val_loss: 17209.2656\n",
      "Epoch 94/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18345.0175 - val_loss: 17209.2656\n",
      "Epoch 95/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18316.8021 - val_loss: 17209.2656\n",
      "Epoch 96/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18329.4267 - val_loss: 17209.2656\n",
      "Epoch 97/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18307.7293 - val_loss: 17209.2656\n",
      "Epoch 98/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18339.0532 - val_loss: 17209.2656\n",
      "Epoch 99/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18231.7012 - val_loss: 17209.2656\n",
      "Epoch 100/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18297.5201 - val_loss: 17209.2656\n",
      "Epoch 101/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18277.8899 - val_loss: 17209.2656\n",
      "Epoch 102/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18317.4955 - val_loss: 17209.2656\n",
      "Epoch 103/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18336.2952 - val_loss: 17209.2656\n",
      "Epoch 104/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18320.3219 - val_loss: 17209.2656\n",
      "Epoch 105/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18346.7440 - val_loss: 17209.2656\n",
      "Epoch 106/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18371.8840 - val_loss: 17209.2656\n",
      "Epoch 107/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18342.3516 - val_loss: 17209.2656\n",
      "Epoch 108/150\n",
      "1053/1053 [==============================] - 2s 1ms/step - loss: 18312.2082 - val_loss: 17209.2656\n",
      "Epoch 109/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18394.3016 - val_loss: 17209.2656\n",
      "Epoch 110/150\n",
      "1053/1053 [==============================] - 2s 1ms/step - loss: 18367.2967 - val_loss: 17209.2656\n",
      "Epoch 111/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18298.5338 - val_loss: 17209.2656\n",
      "Epoch 112/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18406.8026 - val_loss: 17209.2656\n",
      "Epoch 113/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18368.0890 - val_loss: 17209.2656\n",
      "Epoch 114/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18277.7131 - val_loss: 17209.2656\n",
      "Epoch 115/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18340.5492 - val_loss: 17209.2656\n",
      "Epoch 116/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18344.6067 - val_loss: 17209.2656\n",
      "Epoch 117/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18284.9186 - val_loss: 17209.2656\n",
      "Epoch 118/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18352.2168 - val_loss: 17209.2656\n",
      "Epoch 119/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18309.5226 - val_loss: 17209.2656\n",
      "Epoch 120/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18363.5894 - val_loss: 17209.2656\n",
      "Epoch 121/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18309.9802 - val_loss: 17209.2656\n",
      "Epoch 122/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18283.3793 - val_loss: 17209.2656\n",
      "Epoch 123/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18335.8140 - val_loss: 17209.2656\n",
      "Epoch 124/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18296.2338 - val_loss: 17209.2656\n",
      "Epoch 125/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18255.1957 - val_loss: 17209.2656\n",
      "Epoch 126/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18338.8405 - val_loss: 17209.2656\n",
      "Epoch 127/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18361.8911 - val_loss: 17209.2656\n",
      "Epoch 128/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18265.4648 - val_loss: 17209.2656\n",
      "Epoch 129/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18340.4324 - val_loss: 17209.2656\n",
      "Epoch 130/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18287.2106 - val_loss: 17209.2656\n",
      "Epoch 131/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18286.8701 - val_loss: 17209.2656\n",
      "Epoch 132/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18379.1431 - val_loss: 17209.2656\n",
      "Epoch 133/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18346.0618 - val_loss: 17209.2656\n",
      "Epoch 134/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18354.2769 - val_loss: 17209.2656\n",
      "Epoch 135/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18253.2343 - val_loss: 17209.2656\n",
      "Epoch 136/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18363.6741 - val_loss: 17209.2656\n",
      "Epoch 137/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18267.1847 - val_loss: 17209.2656\n",
      "Epoch 138/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18354.1407 - val_loss: 17209.2656\n",
      "Epoch 139/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18272.2963 - val_loss: 17209.2656\n",
      "Epoch 140/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18340.2741 - val_loss: 17209.2656\n",
      "Epoch 141/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18376.1330 - val_loss: 17209.2656\n",
      "Epoch 142/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18292.8983 - val_loss: 17209.2656\n",
      "Epoch 143/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18329.5678 - val_loss: 17209.2656\n",
      "Epoch 144/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18303.7506 - val_loss: 17209.2656\n",
      "Epoch 145/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18362.6375 - val_loss: 17209.2656\n",
      "Epoch 146/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18325.7822 - val_loss: 17209.2656\n",
      "Epoch 147/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18342.8787 - val_loss: 17209.2656\n",
      "Epoch 148/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18343.6900 - val_loss: 17209.2656\n",
      "Epoch 149/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18333.5055 - val_loss: 17209.2656\n",
      "Epoch 150/150\n",
      "1053/1053 [==============================] - 1s 1ms/step - loss: 18303.2154 - val_loss: 17209.2656\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X, Y, validation_split=0.3, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470/470 [==============================] - 0s 672us/step - loss: 17989.7305\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-d8d88c1a336f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s: %.2f%%\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'val_loss'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEWCAYAAABMoxE0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAdfUlEQVR4nO3df7RVdZ3/8edLIX6ogF6wkAtdJs3yVyhXQtPJ/BVoKalfRPOLM19WmKtZmaUp09JyvtNMNqZFlo0loVakoY42ykimhlOIXpQSlb5cCocDBIiAoKJI7+8f+3PgcD0Xz737nnsO8nqsdZb7fD577/M+4OV1P5/POXsrIjAzM+usPWpdgJmZ7docJGZmlouDxMzMcnGQmJlZLg4SMzPLxUFiZma5OEjMuomk6ZL+ucJ9l0o6Oe95zLqDg8TMzHJxkJiZWS4OErMSaUrpckl/kPSKpFskvVvSLEkbJT0kad+S/c+Q9Kyk9ZIelfTBkr4jJT2VjrsD6N3mtT4haUE69neSjuhkzZ+R1CrpJUn3STogtUvSDZJWS9qQ3tNhqe80Sc+l2pZLuqxTf2BmOEjMyjkbOAV4P/BJYBbwj8BAsp+ZzwNIej8wA/gCMAh4APilpHdJehfwH8DtwH7AL9J5ScceBUwDLgIagH8H7pPUqyOFSjoR+FdgPDAYeAH4eeo+Ffjb9D4GAOcCa1PfLcBFEbEPcBjwcEde16yUg8Tsrb4bEasiYjnwGDAvIp6OiNeBe4Aj037nAvdHxK8iYgtwHdAHOBYYDfQEvh0RWyJiJvBkyWt8Bvj3iJgXEVsj4lbg9XRcR3wamBYRT6X6pgDHSGoCtgD7AB8AFBHPR8TKdNwW4BBJ/SJiXUQ81cHXNdvGQWL2VqtKtl8r83zvtH0A2QgAgIj4K7AMGJL6lseOV0V9oWT7vcCX0rTWeknrgaHpuI5oW8MmslHHkIh4GLgR+B6wStLNkvqlXc8GTgNekPQbScd08HXNtnGQmHXeCrJAALI1CbIwWA6sBIaktqJhJdvLgK9HxICSR9+ImJGzhr3IpsqWA0TE1IgYCRxKNsV1eWp/MiLOBPYnm4K7s4Ova7aNg8Ss8+4ETpd0kqSewJfIpqd+B8wF3gQ+L6mHpLOAUSXH/hD4rKQPp0XxvSSdLmmfDtbwM+DvJY1I6yv/QjYVt1TS0en8PYFXgM3A1rSG82lJ/dOU3MvA1hx/Drabc5CYdVJE/BG4APgu8CLZwvwnI+KNiHgDOAv4O2Ad2XrK3SXHtpCtk9yY+lvTvh2t4dfAVcBdZKOg9wETUnc/ssBaRzb9tZZsHQfgfwNLJb0MfDa9D7NOkW9sZWZmeXhEYmZmuThIzMwsFweJmZnl4iAxM7NcetS6gO42cODAaGpqqnUZZma7lPnz578YEYPK9e12QdLU1ERLS0utyzAz26VIeqG9Pk9tmZlZLg4SMzPLxUFiZma5OEjMzCwXB4mZmeXiIDEzs1wcJGZmlstu9z2SzmpZ+hJzFr9Y6zLMzDrtpA/sz4eGDujy8zpIKjT/hXVM/fXiWpdhZtZp++/Ty0FSSxd99H1c9NH31boMM7O64zUSMzPLxUFiZma5OEjMzCwXB4mZmeXiIDEzs1wcJGZmlouDxMzMcnGQmJlZLg4SMzPLxUFiZma5OEjMzCwXB4mZmeXiIDEzs1wcJGZmlouDxMzMcnGQmJlZLg4SMzPLpWpBImmapNWSFpa0jZD0uKQFklokjUrtp0iaL+mZ9N8TS44ZmdpbJU2VpNTeS9IdqX2epKZqvRczM2tfNUck04Exbdq+CVwTESOAq9NzgBeBT0bE4cCFwO0lx9wETAYOSo/iOScB6yLiQOAG4NoqvAczM3sbVQuSiJgDvNS2GeiXtvsDK9K+T0fEitT+LNA7jTgGA/0iYm5EBHAbMC7tdyZwa9qeCZxUHK2YmVn36dHNr/cF4EFJ15GF2LFl9jkbeDoiXpc0BCiU9BWAIWl7CLAMICLelLQBaCAb3exA0mSyUQ3Dhg3rordiZmbQ/YvtFwOXRsRQ4FLgltJOSYeSTVFdVGwqc46ooG/HxoibI6I5IpoHDRrUqcLNzKy87g6SC4G70/YvgFHFDkmNwD3AxIhYkpoLQGPJ8Y2k6bDUNzQd24NsqqztVJqZmVVZdwfJCuCjaftEYDGApAHA/cCUiPhtceeIWAlslDQ6rX9MBO5N3feRBRPAOcDDaR3FzMy6UdXWSCTNAE4ABkoqAF8FPgN8J40gNpPWLYB/AA4ErpJ0VWo7NSJWk02HTQf6ALPSA7JpsdsltZKNRCZU672YmVn7tLv9Et/c3BwtLS21LsPMbJciaX5ENJfr8zfbzcwsFweJmZnl4iAxM7NcHCRmZpaLg8TMzHJxkJiZWS4OEjMzy8VBYmZmuThIzMwsFweJmZnl4iAxM7NcHCRmZpaLg8TMzHJxkJiZWS4OEjMzy8VBYmZmuThIzMwsFweJmZnl4iAxM7NcHCRmZpaLg8TMzHJxkJiZWS4OEjMzy8VBYmZmuThIzMwsFweJmZnl4iAxM7NcqhYkkqZJWi1pYUnbCEmPS1ogqUXSqJK+KZJaJf1R0sdL2kdKeib1TZWk1N5L0h2pfZ6kpmq9FzMza181RyTTgTFt2r4JXBMRI4Cr03MkHQJMAA5Nx3xf0p7pmJuAycBB6VE85yRgXUQcCNwAXFu1d2JmZu2qWpBExBzgpbbNQL+03R9YkbbPBH4eEa9HxJ+BVmCUpMFAv4iYGxEB3AaMKznm1rQ9EzipOFoxM7Pu06ObX+8LwIOSriMLsWNT+xDg8ZL9CqltS9pu2148ZhlARLwpaQPQALzY9kUlTSYb1TBs2LCuei9mZkb3L7ZfDFwaEUOBS4FbUnu5kUTspH1nx7y1MeLmiGiOiOZBgwZ1sGQzM9uZ7g6SC4G70/YvgOJiewEYWrJfI9m0VyFtt23f4RhJPcimytpOpZmZWZV1d5CsAD6atk8EFqft+4AJ6ZNYw8kW1Z+IiJXARkmj0/rHRODekmMuTNvnAA+ndRQzM+tGVVsjkTQDOAEYKKkAfBX4DPCdNILYTFq3iIhnJd0JPAe8CXwuIramU11M9gmwPsCs9IBsWux2Sa1kI5EJ1XovZmbWPu1uv8Q3NzdHS0tLrcswM9ulSJofEc3l+vzNdjMzy8VBYmZmuThIzMwsFweJmZnl4iAxM7NcHCRmZpaLg8TMzHJxkJiZWS4OEjMzy8VBYmZmuThIzMwsFweJmZnl4iAxM7NcHCRmZpaLg8TMzHJxkJiZWS4OEjMzy8VBYmZmuThIzMwslx61LsDMbFewZcsWCoUCmzdvrnUpVdW7d28aGxvp2bNnxcc4SMzMKlAoFNhnn31oampCUq3LqYqIYO3atRQKBYYPH17xcZ7aMjOrwObNm2loaHjHhgiAJBoaGjo86nKQmJlV6J0cIkWdeY8VB4mkYyWdL2li8dHhVzMzs05Zv3493//+9zt83Gmnncb69eurUNF2FQWJpNuB64DjgKPTo7mKdZmZWYn2gmTr1q07Pe6BBx5gwIAB1SoLqHyxvRk4JCKimsWYmVl5V155JUuWLGHEiBH07NmTvffem8GDB7NgwQKee+45xo0bx7Jly9i8eTOXXHIJkydPBqCpqYmWlhY2bdrE2LFjOe644/jd737HkCFDuPfee+nTp0/u2ioNkoXAe4CVlZ5Y0jTgE8DqiDgstd0BHJx2GQCsj4gRknoCPwKOSjXdFhH/mo4ZCUwH+gAPAJdEREjqBdwGjATWAudGxNJK6zMz66xrfvksz614uUvPecgB/fjqJw9tt/8b3/gGCxcuZMGCBTz66KOcfvrpLFy4cNunq6ZNm8Z+++3Ha6+9xtFHH83ZZ59NQ0PDDudYvHgxM2bM4Ic//CHjx4/nrrvu4oILLshd+06DRNIvgQD2AZ6T9ATwerE/Is7YyeHTgRvJ/rEv7n9uybm/BWxIT/8X0CsiDpfUN73WjBQMNwGTgcfJgmQMMAuYBKyLiAMlTQCuBbad38zsnWzUqFE7fER36tSp3HPPPQAsW7aMxYsXvyVIhg8fzogRIwAYOXIkS5cu7ZJa3m5Ecl1nTxwRcyQ1letT9rGA8cCJxd2BvST1IBt5vAG8LGkw0C8i5qbjbgPGkQXJmcDX0vEzgRslydNvZlZtOxs5dJe99tpr2/ajjz7KQw89xNy5c+nbty8nnHBC2Y/w9urVa9v2nnvuyWuvvdYltew0SCLiNwCShgMrI2Jzet4HeHeO1z0eWBURi9PzmWTBsBLoC1waES9JagYKJccVgCFpewiwLNX5pqQNQAPwYtsXkzSZbFTDsGHDcpRtZlYb++yzDxs3bizbt2HDBvbdd1/69u3LokWLePzxx7u1tkrXSH4BHFvyfGtqO7qTr3seMKPk+ah0zgOAfYHHJD0ElPtAc3HEsbO+HRsjbgZuBmhubvaIxcx2OQ0NDXzkIx/hsMMOo0+fPrz73dt/lx8zZgw/+MEPOOKIIzj44IMZPXp0t9ZWaZD0iIg3ik8i4g1J7+rMC6bpq7PIFsmLzgf+KyK2AKsl/Zbsk2KPAY0l+zUCK9J2ARgKFNI5+wMvdaYmM7Ndwc9+9rOy7b169WLWrFll+4rrIAMHDmThwoXb2i+77LIuq6vSLySukbRtYV3SmZSZQqrQycCiiCidsvof4ERl9gJGp31WAhsljU7rKhOBe9Mx9wEXpu1zgIe9PmJm1v0qDZLPAv8oaZmkZcAVpDWH9kiaAcwFDpZUkDQpdU1gx2ktgO8Be5N9zPhJ4McR8YfUdzHZR4NbgSVkC+0AtwANklqBLwJXVvhezMysC1U0tRURS4DRkvYGFBHlV3x2POa8dtr/rkzbJrKPAJfbvwU4rEz75vaOMTOz7lPpJVL6S7oeeBR4RNK3JPWvamVmZrZLqHRqaxqwkey7H+OBl4EfV6soMzPbdVT6qa33RcTZJc+vkbSgGgWZmdmupdIRyWuSjis+kfQRoGu+EmlmZm+rs5eRB/j2t7/Nq6++2sUVbVdpkFwMfE/SUkkvkF1D66KqVWVmZjuo5yCp9FNbC4APSeqXnnftZS/NzGynSi8jf8opp7D//vtz55138vrrr/OpT32Ka665hldeeYXx48dTKBTYunUrV111FatWrWLFihV87GMfY+DAgTzyyCNdXltFQSKpAfgq2Y2tQtJ/A/8UEWu7vCIzs3o360r4yzNde873HA5jv9Fud+ll5GfPns3MmTN54okniAjOOOMM5syZw5o1azjggAO4//77gewaXP379+f666/nkUceYeDAgV1bc1Lp1NbPgTXA2WTfIl8D3FGViszMbKdmz57N7NmzOfLIIznqqKNYtGgRixcv5vDDD+ehhx7iiiuu4LHHHqN//+75lkaln9raLyL+b8nzf5Y0rhoFmZnVvZ2MHLpDRDBlyhQuuuitS9Xz58/ngQceYMqUKZx66qlcffXVVa+n0hHJI5ImSNojPcYD91ezMDMz2670MvIf//jHmTZtGps2bQJg+fLlrF69mhUrVtC3b18uuOACLrvsMp566qm3HFsNlY5ILgIuBW5Pz/cEXpH0RSAiol81ijMzs0zpZeTHjh3L+eefzzHHHAPA3nvvzU9+8hNaW1u5/PLL2WOPPejZsyc33XQTAJMnT2bs2LEMHjy4KovtquSCuZL2AD4NDI+If5I0DBgcEfO6vKIqa25ujpaWllqXYWa7mOeff54PfvCDtS6jW5R7r5LmR0Rzuf0rndr6Html3YsXYtxI9l0SMzPbzVU6tfXhiDhK0tMAEbGusze2MjOzd5ZKRyRbJO1JupWtpEHAX6tWlZmZ7TIqDZKpwD3A/pK+Dvw38C9Vq8rMrA7tDjdh7cx7rPQSKT+VNB84CRAwLiKe7/CrmZntonr37s3atWtpaGggu/P3O09EsHbtWnr37t2h4ypdIyEiFgGLOlqYmdk7QWNjI4VCgTVr1tS6lKrq3bs3jY2NHTqm4iAxM9ud9ezZk+HDh9e6jLpU6RqJmZlZWQ4SMzPLxUFiZma5OEjMzCwXB4mZmeXiIDEzs1wcJGZmlkvVgkTSNEmrJS0sabtD0oL0WCppQUnfEZLmSnpW0jOSeqf2kel5q6SpSl8pldQrna9V0jxJTdV6L2Zm1r5qjkimA2NKGyLi3IgYEREjgLuAuwEk9QB+Anw2Ig4FTgC2pMNuAiYDB6VH8ZyTgHURcSBwA3BtFd+LmZm1o2pBEhFzgJfK9aVRxXhgRmo6FfhDRPw+Hbs2IrZKGgz0i4i5kV1J7DageK/4M4Fb0/ZM4CS9Uy+AY2ZWx2q1RnI8sCoiFqfn7wdC0oOSnpL05dQ+BCiUHFdIbcW+ZQAR8SawAWioeuVmZraDWl1r6zy2j0aKdRwHHA28Cvw6XW345TLHFq9xXG70Ufb6x5Imk02PMWzYsE6WbGZm5XT7iCSth5wF3FHSXAB+ExEvRsSrwAPAUam99DKUjcCKkmOGlpyzP+1MpUXEzRHRHBHNgwYN6sq3Y2a226vF1NbJwKKIKJ2yehA4QlLfFAofBZ6LiJXARkmj0/rHRODedMx9wIVp+xzg4dgd7jpjZlZnqvnx3xnAXOBgSQVJk1LXBHac1iIi1gHXA08CC4CnIuL+1H0x8COgFVgCzErttwANklqBLwJXVuu9mJlZ+7S7/RLf3NwcLS0ttS7DzGyXIml+RDSX6/M3283MLBcHiZmZ5eIgMTOzXBwkZmaWi4PEzMxycZCYmVkuDhIzM8vFQWJmZrk4SMzMLBcHiZmZ5eIgMTOzXBwkZmaWi4PEzMxycZCYmVkuDhIzM8vFQWJmZrk4SMzMLBcHiZmZ5eIgMTOzXBwkZmaWi4PEzMxycZCYmVkuDhIzM8vFQWJmZrk4SMzMLBcHiZmZ5eIgMTOzXKoWJJKmSVotaWFJ2x2SFqTHUkkL2hwzTNImSZeVtI2U9IykVklTJSm190rna5U0T1JTtd6LmZm1r5ojkunAmNKGiDg3IkZExAjgLuDuNsfcAMxq03YTMBk4KD2K55wErIuIA9Nx13Zp9WZmVpGqBUlEzAFeKteXRhXjgRklbeOAPwHPlrQNBvpFxNyICOA2YFzqPhO4NW3PBE4qjlbMzKz71GqN5HhgVUQsBpC0F3AFcE2b/YYAhZLnhdRW7FsGEBFvAhuAhnIvJmmypBZJLWvWrOmyN2FmZrULkvMoGY2QBcgNEbGpzX7lRhhRQd+OjRE3R0RzRDQPGjSow8WamVn7enT3C0rqAZwFjCxp/jBwjqRvAgOAv0raTLaO0liyXyOwIm0XgKFAIZ2zP+1MpZmZWfV0e5AAJwOLImLblFVEHF/clvQ1YFNE3Jieb5Q0GpgHTAS+m3a9D7gQmAucAzyc1lHMzKwbVfPjvzPI/pE/WFJB0qTUNYEdp7XezsXAj4BWYAnbP9V1C9AgqRX4InBllxRuZmYdot3tl/jm5uZoaWmpdRlmZrsUSfMjorlcn7/ZbmZmuThIzMwsFweJmZnl4iAxM7NcHCRmZpaLg8TMzHJxkJiZWS4OEjMzy8VBYmZmuThIzMwsFweJmZnl4iAxM7NcHCRmZpaLg8TMzHJxkJiZWS4OEjMzy8VBYmZmuThIzMwsFweJmZnl4iAxM7NcHCRmZpaLg8TMzHJxkJiZWS4OEjMzy8VBYmZmuThIzMwsl6oFiaRpklZLWljSdoekBemxVNKC1H6KpPmSnkn/PbHkmJGpvVXSVElK7b3S+VolzZPUVK33YmZm7etRxXNPB24Ebis2RMS5xW1J3wI2pKcvAp+MiBWSDgMeBIakvpuAycDjwAPAGGAWMAlYFxEHSpoAXAtsO3+Xm3Ul/OWZqp3ezKzq3nM4jP1Gl5+2aiOSiJgDvFSuL40qxgMz0r5PR8SK1P0s0DuNOAYD/SJibkQEWSiNS/udCdyatmcCJxVHK2Zm1n2qOSLZmeOBVRGxuEzf2cDTEfG6pCFAoaSvwPaRyhBgGUBEvClpA9BANrrpelVIcTOzd4JaBcl5pNFIKUmHkk1RnVpsKnNsVNDX9ryTyabHGDZsWEdrNTOznej2T21J6gGcBdzRpr0RuAeYGBFLUnMBaCzZrRFYUdI3tOSc/WlnKi0ibo6I5ohoHjRoUFe9FTMzozYf/z0ZWBQR26asJA0A7gemRMRvi+0RsRLYKGl0Wv+YCNybuu8DLkzb5wAPp3UUMzPrRtX8+O8MYC5wsKSCpEmpawJvndb6B+BA4KqSjwfvn/ouBn4EtAJLyD6xBXAL0CCpFfgicGW13ouZmbVPu9sv8c3NzdHS0lLrMszMdimS5kdEc7k+f7PdzMxycZCYmVkuDhIzM8tlt1sjkbQGeKGThw+kWl947DqusWu4xq5R7zXWe31QPzW+NyLKfn9itwuSPCS1tLfYVC9cY9dwjV2j3mus9/pg16jRU1tmZpaLg8TMzHJxkHTMzbUuoAKusWu4xq5R7zXWe32wC9ToNRIzM8vFIxIzM8vFQWJmZrk4SCokaYykP6Z7xNfFBSIlDZX0iKTnJT0r6ZLUvp+kX0lanP67b43r3FPS05L+s07rGyBppqRF6c/ymDqs8dL0d7xQ0gxJvWtdo6RpklZLWljS1m5Nkqakn58/Svp4DWv8t/R3/QdJ96Srj9dVjSV9l0kKSQNrWePbcZBUQNKewPeAscAhwHmSDqltVQC8CXwpIj4IjAY+l+q6Evh1RBwE/JraXxn5EuD5kuf1Vt93gP+KiA8AHyKrtW5qTHcK/TzQHBGHAXuSXUW71jVOB8a0aStbU/r/cgJwaDrm++nnqhY1/go4LCKOAP4fMKUOa0TSUOAU4H9K2mpV4045SCozCmiNiD9FxBvAz8nuGV9TEbEyIp5K2xvJ/gEcwo73s7+V7fe573bphmWnk90KoKie6usH/C3ZbQmIiDciYj11VGPSA+iTbuLWl+wGbzWtMSLm8NabybVX05nAzyPi9Yj4M9ltIUbVosaImB0Rb6anj7P95nl1U2NyA/Bldrzza01qfDsOkspsuz98Unrv+LogqQk4EpgHvDvdFKx4c7D92z+y6r5N9sPw15K2eqrvb4A1wI/T9NuPJO1VTzVGxHLgOrLfTFcCGyJidj3VWKK9mur1Z+j/sP0eR3VTo6QzgOUR8fs2XXVTYykHSWUqvj98LUjaG7gL+EJEvFzreookfQJYHRHza13LTvQAjgJuiogjgVeo/VTbDtI6w5nAcOAAYC9JF9S2qg6ru58hSV8hmx7+abGpzG7dXqOkvsBXgKvLdZdpq/m/RQ6Symy7P3xSeu/4mpLUkyxEfhoRd6fmVZIGp/7BwOoalfcR4AxJS8mmA0+U9JM6qg+yv9tCRMxLz2eSBUs91Xgy8OeIWBMRW4C7gWPrrMai9mqqq58hSRcCnwA+XXKL7nqp8X1kvzT8Pv3sNAJPSXoP9VPjDhwklXkSOEjScEnvIlvsuq/GNSFJZHP7z0fE9SVdpfezv5Dt97nvVhExJSIaI6KJ7M/s4Yi4oF7qA4iIvwDLJB2cmk4CnqOOaiSb0hotqW/6Oz+JbD2snmosaq+m+4AJknpJGg4cBDxRg/qQNAa4AjgjIl4t6aqLGiPimYjYPyKa0s9OATgq/b9aFzW+RUT4UcEDOI3sEx5LgK/Uup5U03Fkw9o/AAvS4zSggewTM4vTf/erg1pPAP4zbddVfcAIoCX9Of4HsG8d1ngNsAhYCNwO9Kp1jcAMsjWbLWT/2E3aWU1k0zVLgD8CY2tYYyvZOkPxZ+YH9VZjm/6lwMBa1vh2D18ixczMcvHUlpmZ5eIgMTOzXBwkZmaWi4PEzMxycZCYmVkuDhKzbiJpU61rMKsGB4mZmeXiIDHrZsr8W7q3yDOSzk3tgyXNkbQg9R2v7F4u00v2vbTW9Zu11aPWBZjths4i+zb9h4CBwJOS5gDnAw9GxNfTPSb6pv2GRHYfEkpvwmRWLzwiMet+xwEzImJrRKwCfgMcTXZNt7+X9DXg8MjuMfMn4G8kfTddI6puru5sVuQgMet+5S4FTmQ3OPpbYDlwu6SJEbGObOTyKPA5drxBmFldcJCYdb85wLlp/WMQWXg8Iem9ZPdv+SHZVZ2PSvfq3iMi7gKuIrvEvVld8RqJWfe7BzgG+D3Z1Zu/HBF/SffIuFzSFmATMJHs7nc/llT8pW9KLQo22xlf/dfMzHLx1JaZmeXiIDEzs1wcJGZmlouDxMzMcnGQmJlZLg4SMzPLxUFiZma5/H+BrOKGmDXIagAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.xlabel('loss')\n",
    "plt.ylabel('epoch')\n",
    "plt.legend(['train', 'test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary packages\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dropout\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(learning_rate,dropout_rate,activation_function,init,neuron1,neuron2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuron1,input_dim = 8,kernel_initializer = init,activation = activation_function))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = init,activation = activation_function))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    \n",
    "    adam = Adam(lr = learning_rate)\n",
    "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "\n",
    "model = KerasClassifier(build_fn = create_model,verbose = 0)\n",
    "\n",
    "# Define the grid search parameters\n",
    "\n",
    "batch_size = [10,20,40]\n",
    "epochs = [10,50,100]\n",
    "learning_rate = [0.001,0.01,0.1]\n",
    "dropout_rate = [0.0,0.1,0.2]\n",
    "activation_function = ['softmax','relu','tanh','linear']\n",
    "init = ['uniform','normal','zero']\n",
    "neuron1 = [4,8,16]\n",
    "neuron2 = [2,4,8]\n",
    "\n",
    "# Make a dictionary of the grid search parameters\n",
    "\n",
    "param_grids = dict(batch_size = batch_size,epochs = epochs,learning_rate = learning_rate,dropout_rate = dropout_rate,\n",
    "                   activation_function = activation_function,init = init,neuron1 = neuron1,neuron2 = neuron2)\n",
    "\n",
    "# Build and fit the GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\n",
    "grid_result = grid.fit(X_standardized,y)\n",
    "\n",
    "# Summarize the results\n",
    "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "  print('{},{} with: {}'.format(mean, stdev, param))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
